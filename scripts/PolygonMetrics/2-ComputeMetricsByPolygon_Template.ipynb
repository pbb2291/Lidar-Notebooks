{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00989581-63bb-44d3-bf4f-0070b9eaec19",
   "metadata": {},
   "source": [
    "# Computing Metrics from Clipped Las Files \n",
    "***Davies Lab Lidar Script***<br>\n",
    "Peter Boucher <br>\n",
    "2022/09/23 <br>\n",
    "\n",
    "<p>This is the second step in a 2 part process for clipping las files with a set of polygons (1-ClipLasWithPolygons.ipynb) and then, computing vegetation structure metrics from the las files for each polygon (2-ComputeMetricsByPolygon.ipynb). </p>\n",
    "\n",
    "#### Inputs: \n",
    "- a folder of clipped las files, with each file named by the unique id from the input shapefile\n",
    "    - Note: The input las files need to have a \"Height\" attribute for each point (height above ground)\n",
    "\n",
    "#### Outputs:\n",
    "- an output shapefile with several lidar metric columns (like the input shapefile, but with more columns)\n",
    "- a csv file with a list of all metric outputs\n",
    "\n",
    "## Define User Inputs Below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "976f1cec-14ad-4f6d-b413-fe8bc8575bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "from pathlib import Path\n",
    "import sys\n",
    "# sys.path.append('/n/davies_lab/Lab/LabLidarScripts/bin/')\n",
    "sys.path.append('../../bin/')\n",
    "from LabLidar_Functions import lasClip_IndivFeature, calccover, calcPercentileHeights\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "import laspy\n",
    "import time\n",
    "\n",
    "# # # USER INPUTS\n",
    "\n",
    "# Path to a shapefile (.shp) of polygon features matching the las files\n",
    "# NOTE: This should be the same input shapefile as in 1-ClipLasWithPolygons.ipynb\n",
    "shpf = Path('../data/in/test/shapefile/MpalaForestGEOCanopies_LabLidarTest_EPSG32637.shp')\n",
    "\n",
    "# Input directory of clipped las files to compute metrics with.\n",
    "# NOTE: This should be the same as the 'od' directory from 1-ClipLasWithPolygons.ipynb\n",
    "ld = Path('../data/out/test/clippedlasfiles/')\n",
    "\n",
    "# outdirectory for metric output files\n",
    "od_metrics = Path('../data/out/test/metrics/')\n",
    "\n",
    "# epsg of the shapefile and the las files, as a string\n",
    "# Kruger is 32736 (WGS84 UTM 36S)\n",
    "# Mpala is 32637 (WGS84 UTM 37N)\n",
    "epsg='32637'\n",
    "\n",
    "# feature id column - unique ID for each feature in the shapefile\n",
    "# NOTE: Should match the file names of the las files in 'ld' directory.\n",
    "featureIDcol = 'treeID'\n",
    "\n",
    "# Max height of voxel stacks \n",
    "# NOTE: Set this to be just above max height of your trees in meters.\n",
    "stackheight=10\n",
    "\n",
    "# Vertical step size for metrics\n",
    "# NOTE: This defines the vertical bin size in meters (how \"thick\" each voxel is).\n",
    "metricstep = 0.25\n",
    "\n",
    "# Ground threshold (i.e. below this height treat points as ground) in meters\n",
    "groundthreshold = 0.05\n",
    "\n",
    "# # # END USER INPUTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609a3de6-43b8-42bc-945f-b4035cbbc6e7",
   "metadata": {},
   "source": [
    "### Compute Metrics from Clipped Las Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27dd5c78-4c13-46bd-96a9-2dbdd56757d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function for using parallel processing and calccover function\n",
    "# Note: This reads each feature, then loads the corresponding las file using the feature ID\n",
    "def calcFeatureMetrics_parallel(feature, lasdir=ld, IDcol=featureIDcol, step=metricstep, groundthres=groundthreshold, hmax=stackheight):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    # load corresponding las file using file naming convention from above\n",
    "    # Note: assumes that the featureID number is an integer\n",
    "    lasf = Path(f'{str(lasdir)}/{IDcol}_{int(feature[IDcol])}.las')\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if lasf.exists():\n",
    "\n",
    "        # load points\n",
    "        l = laspy.read(str(lasf))\n",
    "\n",
    "        if len(l.points) > 0:\n",
    "\n",
    "            # Calculate Cover, Percentile Heights, and Point Height Values (defined in \"LabLidar_Functions.py\" fiile)\n",
    "            # Return dictionary outputs\n",
    "            cover = calccover(points=l.points,\n",
    "                              hmin=0, hmax=hmax, step=step, \n",
    "                              heightcol='HeightAboveGround',\n",
    "                              numretcol='number_of_returns',\n",
    "                              retnumcol='return_number',\n",
    "                              classcol='classification',\n",
    "                              groundthres=0.05,\n",
    "                              calcintensity=False)\n",
    "\n",
    "            perc, heights = calcPercentileHeights(l.points, groundthres=groundthres, returnHeights=True, heightcol='HeightAboveGround')\n",
    "\n",
    "            end = time.time()\n",
    "            tottime = end - start\n",
    "\n",
    "        else:\n",
    "\n",
    "            cover = None\n",
    "            perc = None\n",
    "            heights = None\n",
    "            end = time.time()\n",
    "            tottime = end - start\n",
    "            \n",
    "            print(f'No points found in {lasf.name} \\n')\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        cover = None\n",
    "        perc = None\n",
    "        heights = None\n",
    "        end = time.time()\n",
    "        tottime = end - start\n",
    "\n",
    "        print(f'No las file found named: {lasf.name} \\n')\n",
    "\n",
    "    # Return cover dict, percentile dict, and height list (for quick recalculation of anything later)\n",
    "    return cover, perc, heights, tottime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae45fc91-d941-4a57-8cd6-ed3d029f0f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Took 42.3 seconds to compute metrics for 1698 files.\n",
      "\n",
      "Now saving a shapefile...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home02/pbb/.conda/envs/Halo/lib/python3.8/site-packages/geopandas/io/file.py:299: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSaved metric shapefile: ../data/out/test/metrics/treeID_Metrics.shp \n",
      "\n",
      "Now saving cover metrics to csv...\n",
      "\n",
      "\tCoverD1.csv saved in ../data/out/test/metrics\n",
      "\tCoverD2.csv saved in ../data/out/test/metrics\n",
      "\tCoverD1byH.csv saved in ../data/out/test/metrics\n",
      "\tCoverD2byH.csv saved in ../data/out/test/metrics\n",
      "\tFHPD1.csv saved in ../data/out/test/metrics\n",
      "\tFHPD2.csv saved in ../data/out/test/metrics\n",
      "\tNpulses.csv saved in ../data/out/test/metrics\n"
     ]
    }
   ],
   "source": [
    "# Read the shapefile as a geodataframe\n",
    "# Note: Expects a file with polygon features only\n",
    "shpdf = gpd.read_file(shpf)\n",
    "\n",
    "# Make a list of all features in shapefile to iterate through\n",
    "features = [f for i, f in shpdf.iterrows()]\n",
    "\n",
    "# Initiate dictionaries for saving metrics\n",
    "cover_dict = {}\n",
    "perc_dict = {}\n",
    "height_dict = {}\n",
    "for f in features:\n",
    "    cover_dict[f[featureIDcol]] = None\n",
    "    perc_dict[f[featureIDcol]] = None\n",
    "    height_dict[f[featureIDcol]] = None\n",
    "\n",
    "# OutDict for new shapefile\n",
    "outdict = {featureIDcol:[],\n",
    "           'MedianH':[],\n",
    "           'MeanH':[],\n",
    "           'MaxH':[],\n",
    "           'Cover0cm':[],\n",
    "           'CoverG':[]}\n",
    "\n",
    "# Begin!\n",
    "start_tottime = time.time()\n",
    "\n",
    "# set up parallel processing for each polygon feature\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    # for [cover, perc, heights, time] with each feature (feat)\n",
    "    # compute metrics\n",
    "    for c_p_h_t, feat in zip(executor.map(calcFeatureMetrics_parallel, features),\n",
    "                             features):\n",
    "        \n",
    "        # If not None\n",
    "        if c_p_h_t[0]:\n",
    "        \n",
    "            # Fill dictionaries using feature ID\n",
    "            cover_dict[feat[featureIDcol]] = c_p_h_t[0]\n",
    "            perc_dict[feat[featureIDcol]] = c_p_h_t[1]\n",
    "            height_dict[feat[featureIDcol]] = c_p_h_t[2]\n",
    "\n",
    "            # Also, fill a dictionary with useful metrics (cover at 25 cm, median height, max height, etc.)\n",
    "            # for joining with a geodataframe, then export to a shapefile\n",
    "            outdict[featureIDcol].append(feat[featureIDcol])\n",
    "            outdict['MedianH'].append(c_p_h_t[1][50][0])\n",
    "            outdict['MeanH'].append(c_p_h_t[1]['mean'][0])\n",
    "            outdict['MaxH'].append(c_p_h_t[1][100][0])\n",
    "            outdict['Cover0cm'].append(c_p_h_t[0]['CoverD2'][0])\n",
    "            outdict['CoverG'].append(c_p_h_t[0]['CoverD2'][1])\n",
    "\n",
    "        # Prints the time it took per feature\n",
    "        # Comment out if running many features\n",
    "        # print(f'Feature ID {feat[featureIDcol]} done in {c_p_h_t[3]} seconds. \\n')\n",
    "        \n",
    "end_tottime = time.time()\n",
    "\n",
    "tottime = end_tottime - start_tottime\n",
    "tottime = np.array(tottime).round(1)\n",
    "\n",
    "print(f'Done. Took {tottime} seconds to compute metrics for {len(features)} files.\\n')\n",
    "\n",
    "# # #\n",
    "\n",
    "print(f'Now saving a shapefile...\\n')\n",
    "\n",
    "outdf = pd.DataFrame(outdict)\n",
    "shpdf_out = shpdf.merge(outdf, on=featureIDcol)\n",
    "shpdf_out.to_file(f'{od_metrics}/{featureIDcol}_Metrics.shp')\n",
    "\n",
    "print(f'\\tSaved metric shapefile: {od_metrics}/{featureIDcol}_Metrics.shp \\n')\n",
    "\n",
    "# # # \n",
    "\n",
    "print(f'Now saving cover metrics to csv...\\n')\n",
    "\n",
    "for var in ['CoverD1', 'CoverD2', 'CoverD1byH', 'CoverD2byH', 'FHPD1', 'FHPD2', 'Npulses']:\n",
    "    \n",
    "    df_list = []\n",
    "    \n",
    "    for featid in cover_dict.keys():\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            # if there are metrics for this feature id\n",
    "            if cover_dict[featid]: \n",
    "                \n",
    "                if (('byH' in var) | ('FHP' in var)):\n",
    "                    heightbins = cover_dict[featid]['HeightBins'][0:-1]\n",
    "                else:\n",
    "                    heightbins = cover_dict[featid]['HeightBins']\n",
    "\n",
    "                # Make a list of dataframes, and for each df:\n",
    "                # Rows are labelled by height bin\n",
    "                # Cols are labelled by feature ID\n",
    "                df_list.append(pd.DataFrame(cover_dict[featid][var],\n",
    "                                            index=heightbins,\n",
    "                                            columns=[featid]))\n",
    "                \n",
    "            # No points or metrics, fill with zeros\n",
    "            else:\n",
    "                \n",
    "                # Make a list of dataframes, and for each df:\n",
    "                # Rows are labelled by height bin\n",
    "                # Cols are labelled by feature ID\n",
    "                df_list.append(pd.DataFrame(np.zeros(len(heightbins)),\n",
    "                                            index=heightbins,\n",
    "                                            columns=[featid]))\n",
    "\n",
    "        except Exception as e:\n",
    "            \n",
    "            print(f\"{e.__class__} for {featid}: \\n\")\n",
    "            print(f\"\\t{e}\\n\")\n",
    "        \n",
    "    # Merge the dataframes together\n",
    "    df_merged = pd.concat(df_list, axis=1)\n",
    "\n",
    "    # Save the dataframes as csv files\n",
    "    df_merged.to_csv(f'{od_metrics}/{var}.csv')\n",
    "    \n",
    "    print(f'\\t{var}.csv saved in {od_metrics}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20424589-5c8a-4876-a7d1-ef74161b377c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now saving percentile metrics to csv...\n",
      "\n",
      "\tPercentileHeights.csv saved in ../data/out/test/metrics\n"
     ]
    }
   ],
   "source": [
    "print(f'Now saving percentile metrics to csv...\\n')\n",
    "\n",
    "df_merged_list = []\n",
    "\n",
    "for var in [0, 25, 50, 75, 98, 100, 'mean', 'std']:\n",
    "    \n",
    "    df_list = []\n",
    "    \n",
    "    for featid in perc_dict.keys():\n",
    "        \n",
    "        # Make a list of dataframes, and for each df:\n",
    "        # Rows are labelled by percentile metric\n",
    "        # Cols are labelled by feature ID\n",
    "        df_list.append(pd.DataFrame(perc_dict[featid][var],\n",
    "                                    index=[var],\n",
    "                                    columns=[featid]))\n",
    "    # Merge the dataframes together\n",
    "    df_merged = pd.concat(df_list, axis=1)\n",
    "    \n",
    "    # Append to a list\n",
    "    df_merged_list.append(df_merged)\n",
    "\n",
    "# merge again\n",
    "df_merge_merged = pd.concat(df_merged_list, axis=0)\n",
    "    \n",
    "# Save the dataframes as csv files\n",
    "df_merge_merged.to_csv(f'{od_metrics}/PercentileHeights.csv')\n",
    "    \n",
    "print(f'\\tPercentileHeights.csv saved in {od_metrics}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fa7902-e494-4d5c-a87c-336a6c66053b",
   "metadata": {},
   "source": [
    "### DONE!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-Halo]",
   "language": "python",
   "name": "conda-env-.conda-Halo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
