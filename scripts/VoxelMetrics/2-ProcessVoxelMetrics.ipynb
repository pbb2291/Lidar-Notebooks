{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf4cd2cf-c74d-4fb6-b6b6-fd9c151b7766",
   "metadata": {},
   "source": [
    "# Voxelize Lidar Data and Compute Metrics \n",
    "***Davies Lab Lidar Script***<br>\n",
    "Peter Boucher <br>\n",
    "2022/11/28 <br>\n",
    "\n",
    "<p>This is the second step in a 3 part process for 1) clipping las files with a set of polygons (1-ClipLasWithPolygonsforVoxels.ipynb); 2) voxeling lidar data, computing vegetation structure metrics, and outputting a pickle file (2-ProcessVoxelMetrics.ipynb); and 3) outputting the pixel and voxel grids of each metric as geotif or netcdf files for use in qgis and other software (3-OutputVoxelMetrics_Geotiff_NetCDF.ipynb). </p>\n",
    "\n",
    "#### Input files/folders: \n",
    "- a folder of las files to build a grid over\n",
    "    - Note: The input las files need to have a \"HeightAboveGround\" attribute for each point\n",
    "\n",
    "#### Output files:\n",
    "- a series of pickle files that store voxelized lidar metrics\n",
    "\n",
    "## Define User Inputs Below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b252988-9a0e-4860-95af-e2fbfe0a36ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import sys\n",
    "# sys.path.append('/n/home02/pbb/scripts/halo-metadata-server/LabLidarScripts/bin/')\n",
    "sys.path.append('../../bin/')\n",
    "from LabLidar_Functions import calccover, calcPercentileHeights, canopyLayerMetrics\n",
    "from LabLidar_Classes import Cloud\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "from pathlib import Path\n",
    "import laspy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from shapely.geometry import Polygon\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.signal import find_peaks, peak_widths\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# makes matplotlib plots big\n",
    "plt.rcParams['figure.figsize'] = [8, 6]\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "# # # USER INPUTS\n",
    "\n",
    "# Input directory of las files to compute metrics over\n",
    "ld = Path('/n/davies_lab/Lab/LabLidarScripts/data/out/test/nkhulu/las_clipped/')\n",
    "# make las inputs \n",
    "lasinputs = [l for l in ld.glob('*.las')]\n",
    "          \n",
    "# outdirectory for voxelized metric output pickle files\n",
    "od_metrics = Path(f'/n/davies_lab/Lab/LabLidarScripts/data/out/test/nkhulu/pickle_metrics')\n",
    "\n",
    "# EPSG code of the las files, as a string\n",
    "# Kruger is 32736 (WGS84 UTM 36S)\n",
    "# Mpala is 32637 (WGS84 UTM 37N)\n",
    "# Selenkay is 32737 (WGS84 UTM37S)\n",
    "epsg='32736'\n",
    "\n",
    "# Max height of voxel stacks \n",
    "# NOTE: Set this to be just above max height of your trees in meters.\n",
    "stackheight = 20\n",
    "\n",
    "# Horizontal Res of Grid (XY pixel size)\n",
    "xysize = 0.5\n",
    "\n",
    "# Vertical step size for metrics\n",
    "# NOTE: This defines the vertical bin size in meters (how \"thick\" each voxel is).\n",
    "verticalres = 0.5\n",
    "\n",
    "# Set the ground threshold in meters (i.e. below this height treat points as ground).\n",
    "# # # \n",
    "# EXPLANATION:\n",
    "# You can use groundthreshold to account for errors in relative accuracy\n",
    "# For example, if the rel. accuracy of ground is about 0.06 m (6 cm) between flightlines,\n",
    "# You could add in a voxel bin that extends from 0-0.06 m, \n",
    "# treating all points with a height in that range as ground points.\n",
    "# (so any hit below 0.06 m counts as ground).\n",
    "# If you prefer to use all points above 0 m, just set groundthres to 0.\n",
    "# Note: groundthreshold can also work with negative heights,\n",
    "# setting it to -0.05 for instance would treat points \n",
    "# with negative height values (between 0 and -5cm) as ground points.\n",
    "# # # \n",
    "groundthreshold = 0.05\n",
    "\n",
    "# height col\n",
    "heightcol = 'HeightAboveGround'\n",
    "\n",
    "# Set Complexity Metric Parameters\n",
    "# Note: Please leave these as their default values,\n",
    "# unless you have a strong reason to change them. \n",
    "# They have been parameterized for savanna environments. \n",
    "\n",
    "# Set method for calculation of peaks/layers (options = 'kde' or 'gauss1d')\n",
    "method = 'gauss1d'\n",
    "\n",
    "# Set smoothing sigma if using 'gauss1d\"\n",
    "sigma=0.5\n",
    "\n",
    "# set relative height for top of herb layer calc in canopyLayerMetrics\n",
    "rh = 0.9\n",
    "\n",
    "# # # END USER INPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32de226d-8929-4396-8c11-e387280c99b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for li in lasinputs:\n",
    "    if not li.exists():\n",
    "        print(f'Warning: {li} does not exist')\n",
    "\n",
    "if len(lasinputs) < 1:\n",
    "    print(f'Warning: Empty input directory- no las files found')\n",
    "    \n",
    "if not od_metrics.exists():\n",
    "    print('Warning: Output directory does not exist')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0ee2ee-c43b-41ad-85d3-67355f029bc7",
   "metadata": {},
   "source": [
    "### Start Voxelizing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3716cf36-c506-4446-a346-f20553e3ccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make voxel height bins\n",
    "# Calc Cover for height bins\n",
    "nbins = ((stackheight - 0) / verticalres) + 1\n",
    "heightbins = np.linspace(0, stackheight, int(nbins))\n",
    "\n",
    "if groundthreshold > 0:\n",
    "    # insert the groundthres into the array (right above 0)\n",
    "    heightbins = np.insert(heightbins, 1, groundthreshold)\n",
    "if groundthreshold < 0:\n",
    "    # insert the groundthres into the array (right below 0)\n",
    "    heightbins = np.insert(heightbins, 0, groundthreshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b933380-ff5d-4787-b642-19c81478f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function for using parallel processing and calccover function \n",
    "# Notice that is calls lc as the first argument\n",
    "# need to write it this way in order to use concurrent futures parallel processing below\n",
    "def calccover_parallel(index, heightcol='HeightAboveGround'):\n",
    "\n",
    "    # make a True/False array \n",
    "    # for all points within the current grid cell\n",
    "    idx_bool = lc.grid_dict['idx_points'] == lc.grid_dict['idx_cells'][index]\n",
    "    \n",
    "    # Subset Points\n",
    "    p = lc.las.points[idx_bool]\n",
    "\n",
    "    # Get height array\n",
    "    # Note: this is slightly different from the \"heights\" output below\n",
    "    h = p[heightcol]\n",
    "    \n",
    "    # Remove high noise points above the canopy\n",
    "    h = h[h<=stackheight]\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Calculate Cover\n",
    "        cover = calccover(points=p,\n",
    "                          heightbins=heightbins, \n",
    "                          step=verticalres,\n",
    "                          groundthres=groundthreshold,\n",
    "                          heightcol=heightcol,\n",
    "                          hmax=stackheight)\n",
    "        \n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Cover Calc. - {e.__class__} for {lc.lasf}: \\n\")\n",
    "        print(f\"\\t{e}\\n\")\n",
    "\n",
    "    try: \n",
    "        \n",
    "        # Calculate height statistics, and return an array of the point heights above groundthreshold\n",
    "        perc, heights = calcPercentileHeights(points=p,\n",
    "                                              groundthres=groundthreshold,\n",
    "                                              returnHeights=True,\n",
    "                                              heightcol=heightcol,\n",
    "                                              hmax=stackheight)\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Percentile Calc. - {e.__class__} for {lc.lasf}: \\n\")\n",
    "        print(f\"\\t{e}\\n\")\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Compute complexity metrics\n",
    "        complexity = canopyLayerMetrics(h=h,\n",
    "                                        hbins=heightbins,\n",
    "                                        method=method,\n",
    "                                        smoothsigma=sigma,\n",
    "                                        rel_height=rh,\n",
    "                                        groundthreshold=groundthreshold)\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Complexity Calc. - {e.__class__} for {lc.lasf}: \\n\")\n",
    "        print(f\"\\t{e}\\n\") \n",
    "\n",
    "    # Return cover dict, percentile dict, height array, and complexity metrics\n",
    "    return cover, perc, heights, complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "830451c6-981f-4a65-9849-cf6c5f9a5f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded FeatureID_FullLowland cloud, time elapsed: 0.6396870613098145\n",
      "\n",
      "Grid created, time elapsed: 38.55724763870239\n",
      "\n",
      "Starting parallel processing of 608957 pixels.\n",
      "\n",
      "Approximate time to completion of FeatureID_FullLowland.las:\n",
      "\t338.309 hrs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home02/pbb/scripts/halo-metadata-server/LabLidarScripts/bin/LabLidar_Functions.py:766: RuntimeWarning: invalid value encountered in multiply\n",
      "  FHD = -1*np.sum(hcounts_norm*np.log(hcounts_norm, where=hcounts_norm>0))\n",
      "/n/home02/pbb/scripts/halo-metadata-server/LabLidarScripts/bin/LabLidar_Functions.py:766: RuntimeWarning: invalid value encountered in multiply\n",
      "  FHD = -1*np.sum(hcounts_norm*np.log(hcounts_norm, where=hcounts_norm>0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics computed, time elapsed: 1703.898029088974\n",
      "\n",
      "29091145 points gridded into 608957 0.5m pixels in 1819.5346162319183 seconds!\n",
      "\n",
      "\tDone with processing FeatureID_FullLowland.\n",
      "\n",
      "Loaded FeatureID_OpenUpland cloud, time elapsed: 3.9258837699890137\n",
      "\n",
      "Grid created, time elapsed: 21.453166961669922\n",
      "\n",
      "Starting parallel processing of 585285 pixels.\n",
      "\n",
      "Approximate time to completion of FeatureID_OpenUpland.las:\n",
      "\t325.158 hrs\n",
      "Metrics computed, time elapsed: 814.8181273937225\n",
      "\n",
      "16659846 points gridded into 585285 0.5m pixels in 912.5875642299652 seconds!\n",
      "\n",
      "\tDone with processing FeatureID_OpenUpland.\n",
      "\n",
      "Loaded FeatureID_OpenLowland cloud, time elapsed: 4.595439434051514\n",
      "\n",
      "Grid created, time elapsed: 10.943308591842651\n",
      "\n",
      "Starting parallel processing of 329857 pixels.\n",
      "\n",
      "Approximate time to completion of FeatureID_OpenLowland.las:\n",
      "\t183.254 hrs\n",
      "Metrics computed, time elapsed: 233.50211811065674\n",
      "\n",
      "8744791 points gridded into 329857 0.5m pixels in 295.8922257423401 seconds!\n",
      "\n",
      "\tDone with processing FeatureID_OpenLowland.\n",
      "\n",
      "Loaded FeatureID_FullUpland cloud, time elapsed: 3.4851460456848145\n",
      "\n",
      "Grid created, time elapsed: 18.829312086105347\n",
      "\n",
      "Starting parallel processing of 343343 pixels.\n",
      "\n",
      "Approximate time to completion of FeatureID_FullUpland.las:\n",
      "\t190.746 hrs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home02/pbb/scripts/halo-metadata-server/LabLidarScripts/bin/LabLidar_Functions.py:766: RuntimeWarning: invalid value encountered in multiply\n",
      "  FHD = -1*np.sum(hcounts_norm*np.log(hcounts_norm, where=hcounts_norm>0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics computed, time elapsed: 418.7212016582489\n",
      "\n",
      "15128273 points gridded into 343343 0.5m pixels in 483.10809111595154 seconds!\n",
      "\n",
      "\tDone with processing FeatureID_FullUpland.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lasf in lasinputs:\n",
    "\n",
    "    ### STEP 1: Load in Cloud \n",
    "    startproj = time.time()\n",
    "\n",
    "    # Make a las cloud class, and grid it\n",
    "    lc = Cloud(lasf=lasf,\n",
    "               gridsize=xysize,\n",
    "               vsize=verticalres,\n",
    "               heightcol=heightcol,\n",
    "               maxh=stackheight)\n",
    "\n",
    "    # get project string from file name for saving below\n",
    "    projstr = Path(lasf).name.split('.')[0]\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    print(f'Loaded {projstr} cloud, time elapsed: {end - startproj}\\n')\n",
    "    \n",
    "    ### STEP 2: Make the grid\n",
    "    start = time.time()\n",
    "    \n",
    "    lc.makegrid()\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    print(f'Grid created, time elapsed: {end - start}\\n')\n",
    "    \n",
    "    ### STEP 2: Compute Cover, FHP, and Percentiles Metrics Over the Cloud's Grid\n",
    "    start = time.time()\n",
    "\n",
    "    # initialize dictionaries for output \n",
    "    lc.cover_dict = {}\n",
    "    lc.perc_dict = {}\n",
    "    lc.height_dict = {}\n",
    "    lc.complexity_dict = {}\n",
    "\n",
    "    # set the cell indices to loop over in parallel\n",
    "    indices = lc.grid_dict['idx_cells']\n",
    "    \n",
    "    numcells = len(lc.grid_dict['idx_cells'])\n",
    "    \n",
    "    print(f'Starting parallel processing of {numcells} pixels.\\n')\n",
    "    # Assuming 2 seconds for each run, and 40 processes as once (should be a conservative estimate)\n",
    "    print(f'\\tApproximate time to completion of {lasf.name}:{np.round(((numcells/40)*2)/36000, 3)} hrs')\n",
    "\n",
    "    ## Use concurrent futures to compute cover over each cell in parallel\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=None) as executor:\n",
    "        for cphc, x, y in zip(executor.map(calccover_parallel, indices),\n",
    "                                 lc.grid_dict['x_cells'],\n",
    "                                 lc.grid_dict['y_cells']):\n",
    "\n",
    "            try:\n",
    "\n",
    "                # Stick the cover, perc, and heights inside the metrics dictionary\n",
    "                # with x and y location as tuple keys\n",
    "                lc.cover_dict[(x, y)] = cphc[0]\n",
    "                lc.perc_dict[(x, y)] = cphc[1]\n",
    "                lc.height_dict[(x, y)] = np.round(cphc[2], decimals=3)\n",
    "                lc.complexity_dict[(x, y)] = cphc[3]\n",
    "\n",
    "            except Exception as e:\n",
    "\n",
    "                print(f\"Saving metrics error - {e.__class__} for {lc.lasf} on pixel ({x}, {y}): \\n\")\n",
    "                print(f\"\\t{e}\\n\") \n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    print(f'Metrics computed, time elapsed: {end - start}\\n')\n",
    "    \n",
    "    ### STEP 3: SAVE VOXEL & GRID METRICS\n",
    "    \n",
    "    # Save outputs as pickles\n",
    "    # \"Can't open a pickle you don't know\" - there can be malicious pickles, be wary.\n",
    "    with open(f'{od_metrics}/{projstr}_{xysize}mgrid_covermetrics.obj', 'wb') as of:\n",
    "        pickle.dump(lc.cover_dict, of, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open(f'{od_metrics}/{projstr}_{xysize}mgrid_percmetrics.obj', 'wb') as of:\n",
    "        pickle.dump(lc.perc_dict, of, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open(f'{od_metrics}/{projstr}_{xysize}mgrid_heights.obj', 'wb') as of:\n",
    "        pickle.dump(lc.height_dict, of, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    with open(f'{od_metrics}/{projstr}_{xysize}mgrid_complexitymetrics.obj', 'wb') as of:\n",
    "        pickle.dump(lc.complexity_dict, of, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    # DONE\n",
    "    endproj = time.time()\n",
    "    projtime = endproj - startproj\n",
    "    \n",
    "    print(f'{lc.las.header.point_count} points gridded into {numcells} {xysize}m pixels in {projtime} seconds!\\n')\n",
    "    \n",
    "    print(f'\\tDone with processing {projstr}.\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f34b72-3c7a-4b26-8dc9-6344edce46e5",
   "metadata": {},
   "source": [
    "#### DONE - Tests and Benchmarks Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b460c51-6c44-4876-9cdd-f11cb7d7071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Script for Benchmarking time\n",
    "\n",
    "# start = time.time()\n",
    "\n",
    "# # set the cell indices to loop over in parallel\n",
    "# indices = lc.grid_dict['idx_cells'][0:1000]\n",
    "\n",
    "# t = []\n",
    "\n",
    "# ## Use concurrent futures to compute cover over each cell in parallel\n",
    "# with concurrent.futures.ProcessPoolExecutor(max_workers=None) as executor:\n",
    "#     for cphc, x, y in zip(executor.map(calccover_parallel, indices),\n",
    "#                              lc.grid_dict['x_cells'],\n",
    "#                              lc.grid_dict['y_cells']):\n",
    "\n",
    "#         try:\n",
    "            \n",
    "#             # Stick the cover, perc, and heights inside the metrics dictionary\n",
    "#             # with x and y location as tuple keys\n",
    "#             lc.cover_dict[(x, y)] = cphc[0]\n",
    "#             lc.perc_dict[(x, y)] = cphc[1]\n",
    "#             lc.height_dict[(x, y)] = np.round(cphc[2], decimals=3)\n",
    "#             lc.complexity_dict[(x, y)] = cphc[3]\n",
    "\n",
    "#         except Exception as e:\n",
    "\n",
    "#             print(f\"Saving metrics error - {e.__class__} for {lc.lasf} on pixel ({x}, {y}): \\n\")\n",
    "#             print(f\"\\t{e}\\n\") \n",
    "            \n",
    "# print(f'\\t1000 {xysize} m cells takes {time.time() - start} s')\n",
    "# # Note: 1000 0.5 m cells takes 43.7 seconds\n",
    "# So you can't run this over an entire 1 km tile (~4 million 0.5 m pixels)\n",
    "# that would take about 2 days per tile.\n",
    "# running at 1 m pixels takes about half the time (# Note: 1000 1 m cells takes 25.5 seconds)\n",
    "# you have to run it over a smaller section, \n",
    "# or downsize the resolution (that said, 1000 5 m cells takes 24.8 s, so there's not much gain)\n",
    "# either that, or make more of an effort to make the code efficient."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-Halo]",
   "language": "python",
   "name": "conda-env-.conda-Halo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
